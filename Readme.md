# Image Captioning Model
This project is the advanced image captioning system by integrating 
computer vision and natural language processing through deep learning, with a 
particular emphasis on the Nepali language. The system aims to generate accurate, 
meaningful descriptions that reflect objects, their attributes, and relationships within 
images. A CNN-LSTM model is used to process grayscale images through 
Convolutional Neural Networks (CNNs), trained on the flicker8k image dataset, to 
produce descriptive captions that can be converted into speechâ€”enhancing 
accessibility for visually impaired individuals. Further, the project expands into Nepali 
captioning by combining pre-trained CNNs with Recurrent Neural Networks (RNNs). 
To improve scalability and accuracy, the study also introduces a CNN-Transformer 
encoder-decoder model capable of producing real-time captions applicable across 
sectors like education, media, security, and e-commerce. With integrated features such 
as object detection, contextual recognition, and API-based deployment, the system aims 
to overcome language barriers and provide inclusive solutions. Despite requiring high 
computational resources, this work contributes significantly to multilingual AI 
development and strengthens the accessibility of visual information for 
underrepresented communities.   